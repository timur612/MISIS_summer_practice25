# Поиск слов в предложении, относящихся к описанию формулы

Данный проект нацелен на обнаружение слов в предложениях, которые описывают математические формулы. Это достигается посредством двух подходов:
- Дообучения BERT-подобной модели для задачи Token Classification.
- Few-shot prompting с использованием open-source LLM для выделения формул из текста.

## Данные
Исходные данные были предоставлены в формате LaTeX (200 строк). Датасет размечен с использованием правил, представленных в презентации при помощи chatGPT для задачи NER.

## Подходы
- **BERT-подход:** Реализована дообучаемая модель для Token Classification с метками BIO.
- **Few-shot LLM:** Использованы few-shot примеры для выделения формул из текста с помощью выделенного LLM клиента.

## Экспиременты
В экспериментальной части реализованы оба подхода:
- **BERT модель:** 
  - Test F1: 0.9419
  - Test Precision: 0.9431
  - Test Recall: 0.9412
- **LLM подход:**  
  - Test F1: 0.4138
  - Test Precision: 0.4286
  - Test Recall: 0.4000

Также проведено сравнение среди критически некорректно обработанных предложений (см. подробности ниже).

## Results
Модель BERT показала высокую точность, в то время как LLM-подход имеет более низкие показатели. Примеры с ошибками:
- Неправильное выделение фраз формулы для предложений [пример].
- Ошибки в синтаксическом выделении текстовых описаний, когда модель не смогла сопоставить зависимые слова.

## Discussion
- **Сравнение моделей:** 
  - BERT-подход обеспечивает стабильные результаты благодаря дообучению на задаче Token Classification.
  - LLM подход демонстрирует потенциальные возможности только при корректной настройке few-shot примеров.
- **Причины ошибок:** 
  - Некорректное сопоставление токенов в BERT-модели.
  - Ограничения few-shot prompting в LLM.
- **Борьба с переобучением:** 
  - Использовались техники ранней остановки и хранение модели с минимальными значениями ошибки на валидации.

## Conclusions
Проект подтверждает жизнеспособность использования дообучаемой модели для задач NER. Несмотря на высокую эффективность BERT-подхода, LLM требует дальнейшей настройки для устранения ошибок. В будущем возможно объединение обоих подходов для достижения ещё более стабильных результатов.